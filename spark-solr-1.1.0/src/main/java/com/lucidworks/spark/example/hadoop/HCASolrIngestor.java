package com.lucidworks.spark.example.hadoop;

import java.util.Iterator;
import java.util.List;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.log4j.Logger;
import org.apache.solr.client.solrj.impl.CloudSolrClient;
import org.apache.solr.common.SolrInputDocument;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;

import com.google.common.base.Optional;
import com.lucidworks.spark.SolrSupport;
import com.lucidworks.spark.SparkApp;

import scala.Tuple2;

public class HCASolrIngestor implements SparkApp.RDDProcessor {
  
  public static Logger log = Logger.getLogger(HCASolrIngestor.class);

  public String getName() { return "hca-collection"; }

  public Option[] getOptions() {
    return new Option[]{
      OptionBuilder
        .withArgName("PATH")
        .hasArg()
        .isRequired(false)
        .withDescription("HDFS path identifying the Org profile directories / files to index")
        .create("profilePath"),
      
        OptionBuilder
        .withArgName("PATH")
        .hasArg()
        .isRequired(false)
        .withDescription("HDFS path identifying the Org Address directories / files to index")
        .create("addrPath"),
        
        OptionBuilder
        .withArgName("PATH")
        .hasArg()
        .isRequired(false)
        .withDescription("HDFS path identifying the Org Identifiers directories / files to index")
        .create("identifierPath"),
        
        OptionBuilder
        .withArgName("INT")
        .hasArg()
        .isRequired(false)
        .withDescription("Queue size for ConcurrentUpdateSolrClient; default is 1000")
        .create("queueSize"),
      OptionBuilder
        .withArgName("INT")
        .hasArg()
        .isRequired(false)
        .withDescription("Number of runner threads per ConcurrentUpdateSolrClient instance; default is 2")
        .create("numRunners"),
      OptionBuilder
        .withArgName("INT")
        .hasArg()
        .isRequired(false)
        .withDescription("Number of millis to wait until CUSS sees a doc on the queue before it closes the current request and starts another; default is 20 ms")
        .create("pollQueueTime")
    };
  }

  // Benchmarking dataset generated by Solr Scale Toolkit
  private static final String[] profileSchema =
		  	("cust_id,org_nm,non_std_org_nm,org_alt_nm,non_std_org_alt_nm,org_enty_type,org_type_cd,org_sub_type_cd,org_spec_cd,org_sub_spec_cd,bu_id,is_active,ins_time,end_time,last_mod_time,last_mod_by,dqm_flag,dqm_err_detail,acd_flag").split(",");


  private static final String[] identifierSchema =
		    ("record_id,cust_id,addr_id,id_type,id_value,is_active,ins_time,end_time,last_mod_time,last_mod_by,dqm_flag,dqm_err_detail,acd_flag").split(",");

  
  private static final String[] addrSchema =
		    ("addr_id,cust_id,addr_type_cd,addr_line_1_nm,addr_line_2_nm,addr_line_3_nm,addr_line_4_nm,city_nm,state_cd,zip_cd,zip_4_cd,county_cd,country_cd,latitude,longitude,msa,street_po_flag,is_deliverable,best_address_flg,verification_status,non_std_addr_line_1_nm,non_std_addr_line_2_nm,non_std_addr_line_3_nm,non_std_addr_line_4_nm,non_std_city_nm,non_std_state_cd,non_std_country_cd,non_std_county_cd,non_std_zip_cd,non_std_zip_4_cd,addr_build_cd,bu_id,is_active,ins_time,end_time,last_mod_time,last_mod_by,dqm_flag,dqm_err_detail,acd_flag").split(",");
  
  private static final String DELIMITER = "\t";
  private static final String CUSTOMER_ID = "cust_id";
  private static final String ADDRESS_ID = "addr_id";
  private static final String RECORD_ID = "record_id";
  
  public JavaPairRDD<String,SolrInputDocument> makePairRDD(JavaRDD<String> rdd, final String keyPrefix, final String keyField, final String customerIdField, final String[] schema){
	JavaPairRDD<String, SolrInputDocument> pairRDD = null;
	pairRDD = rdd.mapToPair(new PairFunction<String, String, SolrInputDocument>() {

				@Override
				public Tuple2<String, SolrInputDocument> call(String line) throws Exception {
					SolrInputDocument doc = new SolrInputDocument();
					String[] row = line.split(DELIMITER);
					if (row.length != schema.length){
						log.info("Row does not comply with Schema " + line);
						return null;
					}
					for (int c = 0; c < row.length; c++)
						if (row[c] != null && row[c].length() > 0)
							doc.setField(schema[c], row[c]);
					
					//Set record id if key field is not null
					if(keyField != null){
						doc.setField("record_id", keyPrefix+doc.getFieldValue(keyField));
					}
					
					return new Tuple2<String, SolrInputDocument>((String) doc.getFieldValue(customerIdField), doc);
				}
			});
  
  return pairRDD;
  }
  
  
  public int run(SparkConf conf, CommandLine cli) throws Exception {
    JavaSparkContext jsc = new JavaSparkContext(conf);

    String zkHost = cli.getOptionValue("zkHost", "localhost:9983");
    
    String collection = cli.getOptionValue("collection", "hca");
    int queueSize = Integer.parseInt(cli.getOptionValue("queueSize", "1000"));
    int numRunners = Integer.parseInt(cli.getOptionValue("numRunners", "4"));
    int pollQueueTime = Integer.parseInt(cli.getOptionValue("pollQueueTime", "20"));
    
    System.out.println("***************************************************************************");
    log.info("********************************************");
    log.info("zkHost" + zkHost);
    log.info("Collection " + collection);
    log.info("queueSize " + queueSize);
    log.info("numRunners " + numRunners);
    log.info("pollQueueTime " + pollQueueTime);
    log.info("********************************************");
    System.out.println("***************************************************************************");

    
    String profilePath = cli.getOptionValue("profilePath", "hdfs://nameservicedev/business/common/am/data/for_purpose/hive/frozen/hca/search_org_profile");
    String addrPath = cli.getOptionValue("addrPath", "hdfs://nameservicedev/business/common/am/data/for_purpose/hive/frozen/hca/search_org_addr");
    String identifierPath = cli.getOptionValue("identifierPath", "hdfs://nameservicedev/business/common/am/data/for_purpose/hive/frozen/hca/search_org_identifier");
    
    log.info("profilePath " + profilePath);
    log.info("addrPath " + addrPath);
    log.info("identifierPath " + identifierPath);
    
    //Create the Base RDD
    JavaRDD<String> profileRDD = jsc.textFile(profilePath);
    JavaRDD<String> addrRDD = jsc.textFile(addrPath);
    JavaRDD<String> identifierRDD = jsc.textFile(identifierPath);
    
    log.info("Defined the 3 Base RDDs ");
    
    //Create Pair RDD using cust_id field as the key 
    //JavaPairRDD<String,SolrInputDocument> profilePairRDD = makePairRDD(profileRDD, "hca-", CUSTOMER_ID, CUSTOMER_ID, profileSchema);
    
    JavaPairRDD<String,SolrInputDocument> profilePairRDD = profileRDD.mapToPair(new PairFunction<String, String, SolrInputDocument>() {

		@Override
		public Tuple2<String, SolrInputDocument> call(String line) throws Exception {
			SolrInputDocument doc = new SolrInputDocument();
			String[] row = line.split(DELIMITER);
			if (row.length != profileSchema.length){
				log.info("Row does not comply with Schema " + line);
				return null;
			}
			for (int c = 0; c < row.length; c++)
				if (row[c] != null && row[c].length() > 0)
					doc.setField(profileSchema[c], row[c]);
			
			//Set record id 
			doc.setField("record_id", "hca-"+doc.getFieldValue(CUSTOMER_ID));
			
			
			return new Tuple2<String, SolrInputDocument>((String) doc.getFieldValue(CUSTOMER_ID), doc);
		}
	});

    
    
    
    		
    //JavaPairRDD<String,SolrInputDocument> addrPairRDD = makePairRDD(addrRDD, "addr-", ADDRESS_ID,CUSTOMER_ID, addrSchema);
    
    JavaPairRDD<String,SolrInputDocument> addrPairRDD = addrRDD.mapToPair(new PairFunction<String, String, SolrInputDocument>() {

		@Override
		public Tuple2<String, SolrInputDocument> call(String line) throws Exception {
			SolrInputDocument doc = new SolrInputDocument();
			String[] row = line.split(DELIMITER);
			if (row.length != addrSchema.length){
				log.info("Row does not comply with Schema " + line);
				return null;
			}
			for (int c = 0; c < row.length; c++)
				if (row[c] != null && row[c].length() > 0)
					doc.setField(addrSchema[c], row[c]);
			
			//Set record id 
			doc.setField("record_id", "addr-"+doc.getFieldValue(ADDRESS_ID));
			
			
			return new Tuple2<String, SolrInputDocument>((String) doc.getFieldValue(CUSTOMER_ID), doc);
		}
	});

    //JavaPairRDD<String,SolrInputDocument> identifierPairRDD = makePairRDD(identifierRDD, "id-", RECORD_ID, CUSTOMER_ID, identifierSchema);
    JavaPairRDD<String,SolrInputDocument> identifierPairRDD = identifierRDD.mapToPair(new PairFunction<String, String, SolrInputDocument>() {

		@Override
		public Tuple2<String, SolrInputDocument> call(String line) throws Exception {
			SolrInputDocument doc = new SolrInputDocument();
			String[] row = line.split(DELIMITER);
			if (row.length != identifierSchema.length){
				log.info("Row does not comply with Schema " + line);
				return null;
			}
			for (int c = 0; c < row.length; c++)
				if (row[c] != null && row[c].length() > 0)
					doc.setField(identifierSchema[c], row[c]);
			
			//Set record id 
			doc.setField("record_id", "id-"+doc.getFieldValue(RECORD_ID));
			
			
			return new Tuple2<String, SolrInputDocument>((String) doc.getFieldValue(CUSTOMER_ID), doc);
		}
	});

    
    
    log.info("Defined the 3 Pair RDDs ");
    
    JavaPairRDD<String,SolrInputDocument> addrIdentifierPairRDD = addrPairRDD.union(identifierPairRDD);
    
    //log.info("Record count after union - " + addrIdentifierPairRDD.count());
    
    JavaPairRDD<String, Iterable<SolrInputDocument>>  addrIdentifierGrpRDD= addrIdentifierPairRDD.groupByKey();
    //log.info("Number of unique keys after grouping " + addrIdentifierGrpRDD.keys().count());
    
    
    //Left join profile and Address rdd and return a Pair RDD
    JavaPairRDD<String, HCASolrDocument> profileAddrIdRDD = profilePairRDD.leftOuterJoin(addrIdentifierGrpRDD).mapToPair( new PairFunction<Tuple2<String, Tuple2<SolrInputDocument,Optional<Iterable<SolrInputDocument>>>>, String, HCASolrDocument>() {



		@Override
		public Tuple2<String, HCASolrDocument> call(Tuple2<String, Tuple2<SolrInputDocument, Optional<Iterable<SolrInputDocument>>>> record) throws Exception {
			HCASolrDocument doc = new HCASolrDocument();
			
			String customerId = record._1();
			SolrInputDocument document = record._2()._1();
			doc.setProfileDocument(document);
			
			
			Optional<Iterable<SolrInputDocument>> childDocs = record._2()._2();
		
			//log.info("=============Customer Id " + customerId);
			if(childDocs.isPresent()){
				Iterator<SolrInputDocument> iterator = childDocs.get().iterator();
				while(iterator.hasNext()){
					SolrInputDocument childDocument = iterator.next();
					doc.addToChildren(childDocument);
					//log.info("Added child document " + childDocument);
				}
			}else{
				log.info("childDocs not present");
			}
			
			
			log.info("Added all children. Final document - " + document);
			return new Tuple2<String, HCASolrDocument>(customerId, doc);
		}
	});
    
    JavaRDD<SolrInputDocument> finalDocs = profileAddrIdRDD.map(new Function<Tuple2<String,HCASolrDocument>, SolrInputDocument>() {


		@Override
		public SolrInputDocument call(Tuple2<String, HCASolrDocument> record) throws Exception {

			String customerId = record._1();
			SolrInputDocument profileDoc = record._2().getProfileDocument();
			List<SolrInputDocument> children = record._2().getChildren();
			
			//log.info("Adding children docs " + children.size());
			
			if(children.size() >0 ){
				profileDoc.addChildDocuments(children);
			}
			
			//log.info("Final doc " + customerId + "\t" + profileDoc );
			return profileDoc;
		}
    	
	});
    
    /*List< SolrInputDocument> collectedItems = finalDocs.collect();
    log.info("Collection done................");
    for(int i=0; i<collectedItems.size(); i++){
    	SolrInputDocument tuple = collectedItems.get(i);
    	log.info(i +"\t" +tuple);
    }*/
    
    //SolrSupport.streamDocsIntoSolr(zkHost, collection, "id", pairs, queueSize, numRunners, pollQueueTime);
    log.info("calling indexDocs");
    SolrSupport.indexDocs(zkHost, collection, 100, finalDocs );
    log.info("indexDocs called");
    
    // send a final commit in case soft auto-commits are not enabled
    CloudSolrClient cloudSolrClient = SolrSupport.getSolrServer(zkHost);
    log.info("Got cloudSolrClient ");
    cloudSolrClient.setDefaultCollection(collection);
    log.info("Collection set");
    cloudSolrClient.commit(true, true);
    log.info("Commit");
    cloudSolrClient.close();
    log.info("Closed");
	
    return 0;
  }
}


/*
List<Integer> documents = new ArrayList<Integer>();
for(int i=0; i<100; i++){
	documents.add(i);
}
JavaRDD<Integer> idRDD = jsc.parallelize(documents);


JavaRDD<SolrInputDocument> docsRDD = idRDD.map(new Function<Integer, SolrInputDocument>() {

	private static final long serialVersionUID = -5416891081797311147L;

	@Override
	public SolrInputDocument call(Integer i) throws Exception {
		SolrInputDocument document = new SolrInputDocument();
		document.addField("Id", i.intValue());
		document.addField("Type", "Product");
		document.addField("StockNumber", "100-"+i);

		SolrInputDocument childDoc = new SolrInputDocument();
		childDoc.addField("Id", "1.1."+i);
		childDoc.addField("Type", "Review");
		childDoc.addField("TagName", "Rview of product");
		childDoc.addField("TagUser", "Suman");
		childDoc.addField("TagType", "1");
		

		document.addChildDocument(childDoc);
		
		childDoc = new SolrInputDocument();
		childDoc.addField("Id", "1.2."+i);
		childDoc.addField("Type", "Review");
		childDoc.addField("TagName", "Rview of product");
		childDoc.addField("TagUser", "Swetha");
		childDoc.addField("TagType", "2");
		document.addChildDocument(childDoc);


		log.info("Document "+ i +"\t"+document.toString());
		
		return document;
	}
	
});*/

